{"pageProps":{"packageRepo":{"package":{"id":"llm-ask-ai-0.1.0","name":"llm-ask-ai","author":"Bernhard Enders","description":"An Espanso package that enables users to quickly send prompts to a local (e.g. Ollama or LM Studio) or remote LLM API calls (OpenAI standard) and insert the AI-generated response directly into any text field.","title":"LLM ask AI","version":"0.1.0","tags":["LLM","AI","artificial intelligence","prompt","Espanso","Ollama","OpenAI"],"archive_url":"https://github.com/espanso/hub/releases/latest/download/llm-ask-ai-0.1.0.zip","archive_sha256_url":"https://github.com/espanso/hub/releases/latest/download/llm-ask-ai-0.1.0-sha256.txt"},"manifest":{"name":"llm-ask-ai","title":"LLM ask AI","description":"An Espanso package that enables users to quickly send prompts to a local (e.g. Ollama or LM Studio) or remote LLM API calls (OpenAI standard) and insert the AI-generated response directly into any text field.","version":"0.1.0","author":"Bernhard Enders","homepage":{"_tag":"Some","value":"https://github.com/bgeneto/espanso-llm-ask-ai"},"tags":["LLM","AI","artificial intelligence","prompt","Espanso","Ollama","OpenAI"]},"readme":"# llm-ask-ai\n\nAn Espanso package that enables users to quickly send prompts to a local (e.g. Ollama or LM Studio) or remote LLM API calls (OpenAI standard) and insert the AI-generated response directly into any text field.\n\n## Requirements\n\n- [Espanso](https://espanso.org/) installed and running\n- Python 3.9+ installed and available on your system path\n- Required Python packages: `openai` and `python-dotenv` (see `requirements.txt`)\n- Access to a local LLM (such as [Ollama](https://ollama.com/) or [LM Studio](https://lmstudio.ai/)) or a remote OpenAI compatible API (in this case, you will need to provide your API key in the `.env` file)\n\n## Configuration\n\nEdit the `.env` environment file inside the package directory () to set the `API_KEY`, `BASE_URL`and `MODEL`. Example:\n\n```bash\nAPI_KEY=ollama\nBASE_URL=http://localhost:11434/v1\nMODEL=llama3.2\n```\n\n> NOTE: don't forget to pull the Ollama model first. In the case above, just issue the command:\n>\n> `ollama pull llama3.2`\n\n## Usage\n\n- Type the Espanso trigger for this package (`:ask:ai`) in any text field.\n- Enter your desired AI prompt when asked.\n- The AI-generated response will be inserted automatically (this can take several seconds, so choose a small and low latency LLM model).\n\n### Example\n\nType:\n```\n:ask:ai\n```\nThen enter:\n```\nSummarize the following text: ...\n```\nPress the \"Submit\" button or \"CTRL + Enter\" to send the request. The response from your configured LLM will appear in place.\n\n## Troubleshooting\n\n- Make sure `python` (not `python3`) executable is available globally on your system's PATH environment variable and that the required packages are installed.\n- Certain Linux distributions, such as Debian, use only a `python3` executable and don't include a `python` executable or symlink. In this case, you may need to install the `python-is-python3` package (recommended), create a symbolic link, or run a command like this:\n  ```bash\n  sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 10\n  ```\n\n- Make sure you have the necessary Python packages installed globally, either by running `pip install openai python-dotenv` or by using a command like `sudo apt install python3-openai python3-dotenv`.\n- Check that your BASE_URL endpoint, MODEL and API_KEY are correctly set in the `.env` file (located in the Espanso config directory: `%CONFIG%/match/packages/llm-ask-ai/.env`).\n- After sending the request, make sure the cursor doesn’t lose focus and remains blinking at the correct insertion point. If it doesn’t, just click to place it right after the trigger string like this **:ask:ai|**\n- Review Espanso logs for errors.\n\n## License\n\nMIT\n\n## Author\n\nBernhard Enders\n\n## Links\n\n- [Homepage](https://github.com/bgeneto/espanso-llm-ask-ai)\n- [Espanso Documentation](https://espanso.org/docs/)\n","packageYml":[{"name":"package.yml","content":"matches:\n  - trigger: \":ask:ai\"\n    replace: \"{{output}}\"\n    vars:\n      - name: \"prompt\"\n        type: \"form\"\n        params:\n          layout: \"What would you like to ask the Artificial intelligence LLM today?\\n\\n[[text]]\"\n          fields:\n            text:\n              multiline: true\n      - name: \"output\"\n        type: \"script\"\n        params:\n          args:\n            - python\n            - \"%CONFIG%/match/packages/llm-ask-ai/ask-ai.py\"\n            - \"{{prompt.text}}\"\n"}],"license":{"_tag":"None"},"serializedReadme":{"compiledSource":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    blockquote: \"blockquote\",\n    code: \"code\",\n    h1: \"h1\",\n    h2: \"h2\",\n    h3: \"h3\",\n    li: \"li\",\n    p: \"p\",\n    pre: \"pre\",\n    strong: \"strong\",\n    ul: \"ul\",\n    ..._provideComponents(),\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.h1, {\n      children: \"llm-ask-ai\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"An Espanso package that enables users to quickly send prompts to a local (e.g. Ollama or LM Studio) or remote LLM API calls (OpenAI standard) and insert the AI-generated response directly into any text field.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Requirements\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.a, {\n          href: \"https://espanso.org/\",\n          children: \"Espanso\"\n        }), \" installed and running\"]\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Python 3.9+ installed and available on your system path\"\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"Required Python packages: \", _jsx(_components.code, {\n          children: \"openai\"\n        }), \" and \", _jsx(_components.code, {\n          children: \"python-dotenv\"\n        }), \" (see \", _jsx(_components.code, {\n          children: \"requirements.txt\"\n        }), \")\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"Access to a local LLM (such as \", _jsx(_components.a, {\n          href: \"https://ollama.com/\",\n          children: \"Ollama\"\n        }), \" or \", _jsx(_components.a, {\n          href: \"https://lmstudio.ai/\",\n          children: \"LM Studio\"\n        }), \") or a remote OpenAI compatible API (in this case, you will need to provide your API key in the \", _jsx(_components.code, {\n          children: \".env\"\n        }), \" file)\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Configuration\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Edit the \", _jsx(_components.code, {\n        children: \".env\"\n      }), \" environment file inside the package directory () to set the \", _jsx(_components.code, {\n        children: \"API_KEY\"\n      }), \", \", _jsx(_components.code, {\n        children: \"BASE_URL\"\n      }), \"and \", _jsx(_components.code, {\n        children: \"MODEL\"\n      }), \". Example:\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"API_KEY=ollama\\nBASE_URL=http://localhost:11434/v1\\nMODEL=llama3.2\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"NOTE: don't forget to pull the Ollama model first. In the case above, just issue the command:\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.code, {\n          children: \"ollama pull llama3.2\"\n        })\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Usage\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"Type the Espanso trigger for this package (\", _jsx(_components.code, {\n          children: \":ask:ai\"\n        }), \") in any text field.\"]\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Enter your desired AI prompt when asked.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"The AI-generated response will be inserted automatically (this can take several seconds, so choose a small and low latency LLM model).\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"Example\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Type:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        children: \":ask:ai\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Then enter:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        children: \"Summarize the following text: ...\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Press the \\\"Submit\\\" button or \\\"CTRL + Enter\\\" to send the request. The response from your configured LLM will appear in place.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Troubleshooting\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [\"Make sure \", _jsx(_components.code, {\n            children: \"python\"\n          }), \" (not \", _jsx(_components.code, {\n            children: \"python3\"\n          }), \") executable is available globally on your system's PATH environment variable and that the required packages are installed.\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [\"Certain Linux distributions, such as Debian, use only a \", _jsx(_components.code, {\n            children: \"python3\"\n          }), \" executable and don't include a \", _jsx(_components.code, {\n            children: \"python\"\n          }), \" executable or symlink. In this case, you may need to install the \", _jsx(_components.code, {\n            children: \"python-is-python3\"\n          }), \" package (recommended), create a symbolic link, or run a command like this:\"]\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-bash\",\n            children: \"sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 10\\n\"\n          })\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [\"Make sure you have the necessary Python packages installed globally, either by running \", _jsx(_components.code, {\n            children: \"pip install openai python-dotenv\"\n          }), \" or by using a command like \", _jsx(_components.code, {\n            children: \"sudo apt install python3-openai python3-dotenv\"\n          }), \".\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [\"Check that your BASE_URL endpoint, MODEL and API_KEY are correctly set in the \", _jsx(_components.code, {\n            children: \".env\"\n          }), \" file (located in the Espanso config directory: \", _jsx(_components.code, {\n            children: \"%CONFIG%/match/packages/llm-ask-ai/.env\"\n          }), \").\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [\"After sending the request, make sure the cursor doesn’t lose focus and remains blinking at the correct insertion point. If it doesn’t, just click to place it right after the trigger string like this \", _jsx(_components.strong, {\n            children: \":ask:ai|\"\n          })]\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"Review Espanso logs for errors.\"\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"License\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"MIT\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Author\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Bernhard Enders\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Links\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"https://github.com/bgeneto/espanso-llm-ask-ai\",\n          children: \"Homepage\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"https://espanso.org/docs/\",\n          children: \"Espanso Documentation\"\n        })\n      }), \"\\n\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = {\n    ..._provideComponents(),\n    ...props.components\n  };\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"versions":["0.1.0"]},"__N_SSG":true}